{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `spark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `emr` spinup\n",
    "\n",
    "we will want to work with an `emr` `hadoop` cluster for the majority of this class, but it takes about 12 or so minutes to fully spin up, so let's get cracking on that early!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**<div align=\"center\">starting up an `emr` cluster</div>**\n",
    "\n",
    "the below steps will create a cluster which, if we leave on throughout class, will cost a little more than 1 USD.\n",
    "\n",
    "+ in the `aws` web console open the `emr` service\n",
    "+ click create cluster, and on the \"quick options\" screen select \"advanced options\"\n",
    "+ software and steps\n",
    "    + stay at emr-5.19\n",
    "    + for software, click:\n",
    "        + `hadoop`\n",
    "        + `ganglia`\n",
    "        + `hive`\n",
    "        + `hue`\n",
    "        + `spark`\n",
    "        + `livy`\n",
    "        + `pig`\n",
    "    + notice but don't click: `jupyterhub`, `mxnet`, `tensorflow`\n",
    "    + click next\n",
    "+ hardware config -- leave all defaults\n",
    "    + generally, think about space requirements [ala this](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-plan-instances-hdfs)\n",
    "+ general options -- pick a name\n",
    "+ security options -- ***choose a key pair!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `spark`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "while it *is* actually still somewhat common to write `hadoop streaming` `python` scripts for doing `etl` work, it's really not great for data science.\n",
    "\n",
    "as you've seen, it's pretty orchestrated and low-level. we're thinking hard about the simple things we want to do (like count words or calculate averages), and when we've figured them out, we're only doing them *once*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "think about how that looks when we want to move on to something more complicated, like a gradient descent algorithm.\n",
    "\n",
    "or anything iterative, for that matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. take parameters and applying a model defined by those parameters to every record in our `hdfs` dataset -- map records of features to predicted `y` values, calcualte individual `y` error term and gradients for each record, and `emit` those\n",
    "2. reduce those partial gradients to determine the parameter update\n",
    "3. update paraemters\n",
    "\n",
    "each time we move between steps we are reading and writing to `hdfs` and that can be crazy wasteful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "also, this is 2017. we should expect that someone has just done this for us. that's fair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the name of the game in data science applications is `spark`.\n",
    "\n",
    "`spark` is a `scala` (an OO functional programming language which runs on the `jvm`) application which is a fast query and iterative algorithm calculation platform. it was built as a replacement for `mapreduce` for calculation workflows just like the ones in which we are most interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "perhaps most importantly for us: there are libraries for the `spark` `api` in `python` and `R`, and this has lead to pretty wide adoption in the communities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### the `spark` stack\n",
    "\n",
    "`spark` ignores data management and focuses exclusively on resource management.\n",
    "\n",
    "the primary programmer interface is the `spark` core api module (i.e. the standard library of `spark`), and this library is entirely focused on implementing commong computation tasks (file `io`, `mapping`, `reducing` , `filtering`) in as efficient a way as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "just like in `python` and `R`, after the core language has taken care of the low-level stuff, specialized tools are built on top of this. in `spark`, some of the most important are:\n",
    "\n",
    "+ `spark sql`: an implementation of the `sql` standard against `rdd`s. dirty.\n",
    "+ `spark streaming`: unbounded data stream processing\n",
    "+ `mllib` and `mahout`: common machine learning algorithms implemented\n",
    "    + `spark-sklearn` is an implementation of the ubiquitous `sklearn` `api` with `mllib` implementation under the hood\n",
    "+ `graphx`: manipulate and calculate graphs\n",
    "+ `zeppelin`: the `jupyter notebook` of the `spark` world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### resilient distributed datasets\n",
    "\n",
    "the fundamental data structure of `spark` is a resilient distributed dataset (`rdd`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "previously we cited the requirements of distributed computing frameworks to be *fault tolerance, recoverability, consistency, and scalability*\n",
    "\n",
    "`rdd`s are `spark`'s way of performing distributed computation while hitting those requirements. at the simplest level, `spark` takes a functional plan of attack (a sequence of functions) and figures out how to distribute the data to many different nodes (in memory) to optimize that plan.\n",
    "\n",
    "this is very similar to the `tensorflow` execution graph -- delay computation until the whole roadmap is defined and the users asks for something specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "some important facts about `rdd`s\n",
    "\n",
    "+ `rdd`s are immutable, read-only collections of objects\n",
    "+ they can be built from a lineage (a series of `fpl` function calls)\n",
    "    + this makes them *fault tolerant, recoverable, consistent*\n",
    "+ they work in parallel, so *scalable*\n",
    "+ they are operated on by `scala`, and `fpl`, so *consistent*\n",
    "+ they are immutable, so *recoverable*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "when you're using the `spark` api, then, your basic abilities are to create, transform, or export these `rdd`s.\n",
    "\n",
    "you need to shift paradigms into a functional mindset: think of things you can do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`spark` breaks these things down into basically two types\n",
    "\n",
    "1. *transformations*: `rdd` $\\rightarrow$ new `rdd`\n",
    "    + `map`: take a big `rdd`, apply something, create a new `rdd` as a result\n",
    "2. *actions*: return something back to the client (aggregation, e.g.)\n",
    "    + `reduce`: repartition `rdd` by key, aggregate (sum, mean)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### programming with `rdd`s\n",
    "\n",
    "the way we actually deploy programs in `spark` is similar to how we deployed `mapreduce` jobs in `hadoop streaming`: we write some code, send it to some local machine, that distributes the computation elsewhere\n",
    "\n",
    "what changes in `spark` is that a master program (the \"driver\") creates `rdd`s by *parallelizing* a `hadoop` dataset (that is, it partitions a given dataset and pushes those partitions to nodes that perform local computations in memory).\n",
    "\n",
    "an `rdd` is a structure that manages this partitionting / parallelizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "from the point of view of the `spark` program, the order of operations is\n",
    "\n",
    "1. build `rdd`s\n",
    "    + access data from `hdfs` or local disk storage\n",
    "    + parallelize that collection of data\n",
    "    + transform it as necessary\n",
    "    + cache everything we can\n",
    "2. pass *closures* (stateless functions, ignorant of the rest of the world) to each element of the `rdd`\n",
    "    + *closures* are then locally applied in-memory and the outputs are also cached\n",
    "3. output `rdd`s are *acted on* (aggregated)\n",
    "    + this is the only place we atually have an eval step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "one quick note on some common terms: *variables* and *closures*\n",
    "\n",
    "+ *closures* do not rely in any way on external data\n",
    "    + if they have variables within, they are copied to the nodes with them, but kept in local scope\n",
    "+ external data, if needed, is passed through shared variables\n",
    "    + *broadcast* variables: read only, distributed (e.g. lookup tables / stopword lists)\n",
    "    + *accumulators*: meant to be associatively updated (e.g. counters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### interactive `spark`\n",
    "\n",
    "`spark` itself is based on `java`, which means it is a compiled language at its heart. however, most development work with `spark` is done in interactive `repl` sessions.\n",
    "\n",
    "`pyspark` is a `repl` for the `spark` api bindings in `python`, so if you want to code `spark` programs using `python`, this is your starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "just like with the `python` language, there are a few different ways you could execute `pyspark` commands:\n",
    "\n",
    "+ in a terminal shell via the `pyspark` command\n",
    "+ in a notebook via several options\n",
    "    + `zeppelin`\n",
    "    + amazon `emr` `notebooks`\n",
    "    + extension kernels for `jupyter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "we are going to launch both the `pyspark` command line version and an amazon `notebook` just to demonstrate how, but in the lecture we will follow the `notebook`.\n",
    "\n",
    "the underlying execution -- using `pyspark` as a kernel -- is the same for both methods, so feel free to follow along executing the following commands using whichever interface you prefer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "in our quick walkthrough we will do the canonical `hadoop` example (word count) again. we will find the frequency of words in the `shakespeare.txt` file. you'll see that the syntax is pretty familiar from traditional `python`, with a few twists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `pyspark` `cli`\n",
    "\n",
    "first, let's open the command line `repl`. on the `emr` master node (`ssh -i $KEY_PAIR hadoop@$MASTER_DNS`)\n",
    "\n",
    "```bash\n",
    "cd ~/code/hadoop-fundamentals\n",
    "# you may get errors!\n",
    "pyspark\n",
    "```\n",
    "\n",
    "verify it worked:\n",
    "\n",
    "```py\n",
    "sc.version\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "if you would like to execute code in this `cli`, leave it open. otherwise, exit with `exit()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `emr` notebook\n",
    "\n",
    "this is a special `aws`-specific thing (don't confuse with `jupyter` or `zeppelin`. notes are [here](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-notebooks-create.html)\n",
    "\n",
    "notes\n",
    "\n",
    "+ notebooks are saved to `s3`; create an `s3` bucket for them\n",
    "+ `aws` `emr` web console > notebooks (left menu)\n",
    "+ create notebook\n",
    "+ update name, description, cluster, and `s3` bucket (leave rest as defaults)\n",
    "+ click open button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Shakespeare word frequency demo\n",
    "\n",
    "let's go through some simple `pyspark` commands in one of our two `repl` environments (`pyspark` or an amazon `emr` `notebook` with a `pyspark`-kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "to start, we use the `sc` spark context object (built as part of the `pyspark` `repl` to create a hook to the Shakespeare text file we loaded into `hdfs` long ago\n",
    "\n",
    "```python\n",
    "text = sc.textFile('/data/shakespeare.txt')\n",
    "print(text)\n",
    "help(text)\n",
    "\n",
    "# look at flatMap\n",
    "help(text.flatMap)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "we can use `flatMap` to apply a tokenization function to every text string:\n",
    "\n",
    "```python\n",
    "# make a tokenize function\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "# looks good. let's tokenize our text into words\n",
    "words = text.flatMap(tokenize)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "we can take that flat list of words and do our standard map and reduce. before we move on here, though, look at the linear (the defining sequence of functions) via `wc.toDebugString`\n",
    "\n",
    "```python\n",
    "# let's apply a map function for word counts\n",
    "wc = words.map(lambda x: (x, 1))\n",
    "\n",
    "# you can see the lineage:\n",
    "print(wc.toDebugString())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "finally, we'll use built-in functions to do our summation `reduce` step\n",
    "\n",
    "```python\n",
    "# include a reduce step to sort/shuffle/partition by key and add the values\n",
    "from operator import add\n",
    "cts = wc.reduceByKey(add)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "finally, save the result to file. note that this is when something actually *happens*. up until this point, we were merely defining a *lineage*; now we're actually asking that some *action* take place, and `spark` springs into action\n",
    "\n",
    "```python\n",
    "\n",
    "# finally do something with it all\n",
    "cts.saveAsTextFile('wc')\n",
    "\n",
    "# exit so we can see the results\n",
    "exit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```bash\n",
    "# look at them beautiful results\n",
    "hadoop fs -ls /tmp/wc/\n",
    "hadoop fs -cat /tmp/wc/part-00000 | head -n25\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**<div align=\"center\">what are your questions so far?</div>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<strong><em><div align=\"center\"><code>s = 'spark'; s.replace('a', 'o')</code></div></em></strong>\n",
    "<div align=\"center\"><img width=300 src=\"https://images-na.ssl-images-amazon.com/images/I/61u0oKyy3wL._SX466_.jpg\"></div>\n",
    "\n",
    "# END OF LECTURE"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
